{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0acc9c",
   "metadata": {},
   "source": [
    "# Assistant de Recherche pour la Génération Automatique de README"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e2a72",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "\n",
    "Dans le monde du développement logiciel, la documentation est souvent reléguée au second plan, malgré son rôle central. Un README clair et complet est pourtant la première impression qu’un utilisateur ou un contributeur aura d’un projet : il facilite la prise en main, l’installation et la compréhension des fonctionnalités. Pourtant, écrire et maintenir une documentation à jour est long, fastidieux et facilement négligé, surtout dans des projets en constante évolution.\n",
    "\n",
    "Ce projet propose une solution révolutionnaire pour tous les développeurs : un assistant automatique capable d’analyser un projet complet et de générer un README.md structuré, cohérent et fidèle à l’état actuel du code.\n",
    "\n",
    "Les bénéfices sont immédiats et concrets :\n",
    "- Gain de temps massif : fini les heures passées à rédiger manuellement un README pour chaque projet ou chaque mise à jour.\n",
    "- Documentation toujours à jour : en analysant directement le code source, l’outil élimine le phénomène de \"documentation drift\" où les README deviennent obsolètes par rapport au code réel.\n",
    "- Accessibilité et collaboration : un README clair permet à n’importe quel collaborateur ou utilisateur de comprendre et de contribuer rapidement au projet.\n",
    "- Polyvalence : même les projets multi-langages ou peu commentés peuvent bénéficier d’une documentation complète et professionnelle.\n",
    "\n",
    "En automatisant ce processus, ce projet ne se contente pas de simplifier la vie des développeurs : il change leur manière de travailler, en rendant la documentation plus rapide, fiable et accessible. \n",
    "\n",
    "Imaginez des milliers de projets à travers le monde, chacun avec un README précis et clair, généré automatiquement. Ce projet a le potentiel de transformer la manière dont les développeurs documentent leurs projets, tout en améliorant la collaboration et la maintenance logicielle à grande échelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6face",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdafeb17",
   "metadata": {},
   "source": [
    "### Paramétrage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5eefbf",
   "metadata": {},
   "source": [
    "1. Importation des bibliothèques nécessaires :\n",
    "   \n",
    "On commence par importer toutes les bibliothèques utiles pour le projet.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37af9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires pour le projet.\n",
    "\n",
    "import os            # Pour les opérations système (gestion des chemins, etc.)\n",
    "import re            # Pour les expressions régulières (utilisé pour le chunking)\n",
    "import json          # Pour la gestion des fichiers JSON (notamment les .ipynb)\n",
    "import numpy as np   # Pour les opérations numériques (essentiel pour les embeddings)\n",
    "from dotenv import load_dotenv # Pour charger la clé API depuis un fichier .env\n",
    "from mistralai import Mistral # SDK officiel pour interagir avec Mistral AI\n",
    "import faiss         # Bibliothèque d'indexation vectorielle (recherche d'embeddings)\n",
    "import time          # Pour la gestion des pauses (retry en cas d'erreur 429)\n",
    "import random        # Pour la gestion des pauses aléatoires (backoff jitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e234510",
   "metadata": {},
   "source": [
    "2. Récupération de la clé API pour l'appel à Miastral AI.\n",
    "\n",
    "Cette étape est cruciale pour sécuriser la clé API et permettre l’accès aux services Mistral AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76449ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe24625",
   "metadata": {},
   "source": [
    "3. Exclusion de fichiers/dossiers inutiles\n",
    "\n",
    "Le pipeline n’analyse que le code pertinent. On exclut donc :\n",
    "- Les environnements virtuels, caches, artefacts de build, fichiers binaires.\n",
    "- Les fichiers temporaires et images inutiles à l’analyse.\n",
    "\n",
    "Cela améliore les performances et réduit le nombre de chunks inutiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1562080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des dossiers à exclure (Python, Node.js, Java, etc.)\n",
    "excluded_dirs = {\n",
    "    # Python\n",
    "    \"venv\", \".venv\", \"__pycache__\", \"site-packages\", \"env\", \".env\",\n",
    "    # Node.js / JS\n",
    "    \"node_modules\", \"bower_components\", \".npm\", \".yarn\",\n",
    "    # Java / JVM\n",
    "    \"target\", \"build\", \".gradle\", \".mvn\", \"out\",\n",
    "    # C/C++ / Rust / Go / .NET\n",
    "    \"cmake-build-debug\", \"cmake-build-release\", \"bin\", \"obj\", \"pkg\", \"dist\",\n",
    "    \"Debug\", \"Release\",\n",
    "    # Divers IDE / SCM\n",
    "    \".git\", \".svn\", \".hg\", \".idea\", \".vscode\"\n",
    "}\n",
    "\n",
    "# Liste des fichiers à exclure (souvent générés automatiquement ou inutiles à l'analyse)\n",
    "excluded_files = {\n",
    "    # SCM / VCS\n",
    "    \".gitignore\", \".gitattributes\", \".gitmodules\",\n",
    "    \".hgignore\", \".svnignore\",\n",
    "\n",
    "    # Config / lockfiles\n",
    "    \"package-lock.json\", \"yarn.lock\", \"pnpm-lock.yaml\",\n",
    "    \"poetry.lock\", \"Pipfile.lock\",\n",
    "\n",
    "    # Build / cache\n",
    "    \"Thumbs.db\", \"Desktop.ini\",\n",
    "    \".DS_Store\",  # macOS\n",
    "    \"npm-debug.log\", \"yarn-error.log\",\n",
    "    \"Cargo.lock\", \"Gemfile.lock\",\n",
    "\n",
    "    # Environnements\n",
    "    \".env\", \".env.local\", \".env.production\", \".env.development\",\n",
    "\n",
    "    # Binaires / artefacts\n",
    "    \"*.pyc\", \"*.pyo\", \"*.pyd\",\n",
    "    \"*.class\", \"*.jar\", \"*.war\", \"*.ear\",\n",
    "    \"*.dll\", \"*.so\", \"*.dylib\",\n",
    "    \"*.exe\", \"*.out\", \"*.o\", \"*.obj\",\n",
    "    \"*.a\", \"*.lib\",\n",
    "\n",
    "    # Archives\n",
    "    \"*.zip\", \"*.tar\", \"*.gz\", \"*.bz2\", \"*.rar\",\n",
    "\n",
    "    # Divers\n",
    "    \"README.md\", \"LICENSE\", \"COPYING\", \"CHANGELOG\", \"TODO\", \"Makefile\",\n",
    "    \n",
    "    # Photos \n",
    "    \"*.png\", \"*.jpg\", \"*.jpeg\", \"*.gif\", \"*.bmp\", \"*.svg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b244c8",
   "metadata": {},
   "source": [
    "## Fonctions principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbec4cb",
   "metadata": {},
   "source": [
    "1. Affichage de l’arborescence (show_tree)\n",
    "\n",
    "Cette fonction affiche l’arborescence du projet jusqu’au second niveau seulement.\n",
    "\n",
    "Cela évite d’afficher des milliers de fichiers pour les gros projets. Et donc de se limiter au premier et second niveaux dans l'arborescence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f809a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tree(root):\n",
    "    \"\"\"\n",
    "    Affiche :\n",
    "    - Tous les fichiers du premier niveau\n",
    "    - Tous les fichiers du second niveau\n",
    "    - Tous les sous-dossiers du second niveau indiqués avec '...'\n",
    "    - Ne descend pas au troisième niveau ou plus\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        # Filtrage des dossiers exclus\n",
    "        dirnames[:] = [d for d in dirnames if d not in excluded_dirs]\n",
    "        filenames[:] = [f for f in filenames if f not in excluded_files]\n",
    "\n",
    "        # Niveau dans l'arborescence\n",
    "        level = dirpath.replace(root, \"\").count(os.sep)\n",
    "        indent = \" \" * 4 * level\n",
    "\n",
    "        # Nom du dossier courant\n",
    "        lines.append(f\"{indent}{os.path.basename(dirpath)}/\")\n",
    "\n",
    "        if level == 0:\n",
    "            # Fichiers du root\n",
    "            for f in filenames:\n",
    "                lines.append(f\"{indent}    {f}\")\n",
    "\n",
    "            # Sous-dossiers niveau 1\n",
    "            for d in dirnames:\n",
    "                lines.append(f\"{indent}    {d}/\")\n",
    "\n",
    "        elif level == 1:\n",
    "            # Fichiers du second niveau\n",
    "            for f in filenames:\n",
    "                lines.append(f\"{indent}    {f}\")\n",
    "\n",
    "            # Sous-dossiers du second niveau → seulement \"...\"\n",
    "            for d in dirnames:\n",
    "                lines.append(f\"{indent}    {d}/ ...\")\n",
    "\n",
    "        else:\n",
    "            dirnames[:] = []  # empêche d'aller plus loin\n",
    "\n",
    "        # Empêche la descente au-delà du second niveau\n",
    "        if level >= 2:\n",
    "            dirnames[:] = []\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34440f6",
   "metadata": {},
   "source": [
    "2. Lecture du code (read_code)\n",
    "\n",
    "Cette fonction lit uniquement le code exploitable :\n",
    "- Pour les .ipynb, seules les cellules de type \"code\" sont conservées.\n",
    "- Pour les autres fichiers, tout le contenu est récupéré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888448b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_code(file_path):\n",
    "    \"\"\"\n",
    "    Lit le contenu d'un fichier de code et renvoie uniquement le code exploitable.\n",
    "\n",
    "    - Pour les fichiers Jupyter Notebook (.ipynb) : \n",
    "      ne conserve que le contenu des cellules de type 'code', et les concatène en une seule chaîne de caractères.\n",
    "    - Pour les autres fichiers de code (.py, .js, .cpp, etc.) :\n",
    "      lit l'intégralité du fichier et retourne son contenu brut.\n",
    "\n",
    "    Paramètre :\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Chemin vers le fichier à lire.\n",
    "\n",
    "    Retour :\n",
    "    -------\n",
    "    str\n",
    "        Contenu du code du fichier, prêt à être utilisé pour le traitement ou\n",
    "        pour la génération de chunks dans un pipeline de documentation.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if ext == \".ipynb\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            nb = json.load(f)\n",
    "            content = []\n",
    "            for cell in nb.get(\"cells\", []):\n",
    "                if cell.get(\"cell_type\") == \"code\":\n",
    "                    content.append(\"\".join(cell.get(\"source\", [])))\n",
    "            return \"\\n\".join(content)\n",
    "\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00662dc",
   "metadata": {},
   "source": [
    "## Découpage du code en chunks + embedding\n",
    "\n",
    "1. Découpage du code en chunks\n",
    "\n",
    "Nous avons donc deux fonctions qui travaillent ensemble:\n",
    "- split_functions : découpe le code en fonctions ou blocs logiques.\n",
    "- split_long_chunk : si un chunk est trop long, on le découpe en morceaux plus petits.\n",
    "\n",
    "Cela permet d'assurer que chaque chunk respecte la limite maximale pour l’API d’embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2edc8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_functions(code):\n",
    "    \"\"\"Découpe le code en fonctions/chunks (première étape de chunking logique).\"\"\"\n",
    "    # Regex : recherche de motifs de fonctions 'def nom(...):' et capture leur corps.\n",
    "    # re.DOTALL (ou S) est crucial pour que le '.' matche les sauts de ligne à l'intérieur de la fonction.\n",
    "    pattern = r\"(def [\\w_]+\\s*\\([^)]*\\):(?:\\n(?:\\s+.+))*)\" \n",
    "    chunks = re.findall(pattern, code, re.DOTALL)\n",
    "    # Si aucune fonction (ex: script simple ou code dans .ipynb), le chunk est le fichier entier.\n",
    "    return chunks if chunks else [code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a10f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille max en caractères pour un chunk envoyé aux embeddings\n",
    "MAX_CHARS_PER_CHUNK = 5000\n",
    "\n",
    "def split_long_chunk(chunk, max_chars=MAX_CHARS_PER_CHUNK):\n",
    "    \"\"\"\n",
    "    Si un chunk est trop long, on le découpe en morceaux plus petits basés sur les lignes.\n",
    "    On essaye de garder des morceaux cohérents tout en respectant la limite de taille.\n",
    "    \"\"\"\n",
    "    if len(chunk) <= max_chars:\n",
    "        return [chunk]\n",
    "\n",
    "    lines = chunk.splitlines()\n",
    "    parts = []\n",
    "    current = []\n",
    "    current_len = 0\n",
    "\n",
    "    for line in lines:\n",
    "        # +1 pour le saut de ligne\n",
    "        extra = len(line) + 1\n",
    "        # Si on dépasse la limite, on coupe ici\n",
    "        if current_len + extra > max_chars and current:\n",
    "            parts.append(\"\\n\".join(current))\n",
    "            current = [line]\n",
    "            current_len = extra\n",
    "        else:\n",
    "            current.append(line)\n",
    "            current_len += extra\n",
    "\n",
    "    if current:\n",
    "        parts.append(\"\\n\".join(current))\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3325f7",
   "metadata": {},
   "source": [
    "2. Création des embeddings\n",
    "\n",
    "Chaque chunk est transformé en vecteur numérique grâce au modèle codestral-embed.\n",
    "\n",
    "Cela permet d'avoir une bonne gestion des erreurs 429 avec backoff exponentiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9afe115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(chunks, batch_size=16, max_retries=5):\n",
    "    \"\"\"\n",
    "    Génère les vecteurs d'embeddings avec gestion du Rate Limit (429) via\n",
    "    Backoff exponentiel + Jitter pour la robustesse des appels API.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        # Traitement par lots (batching) pour optimiser les appels API.\n",
    "        batch = chunks[i:i+batch_size] \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Appel à l'API Mistral AI (modèle codestral-embed)\n",
    "                resp = client.embeddings.create( \n",
    "                    model=\"codestral-embed\",\n",
    "                    inputs=batch\n",
    "                )\n",
    "                # Conversion en numpy array pour l'indexation FAISS\n",
    "                for emb in resp.data: \n",
    "                    embeddings.append(np.array(emb.embedding, dtype=np.float32))\n",
    "                break # Succès : on sort de la boucle de retry\n",
    "\n",
    "            except Exception as e:\n",
    "                # Gestion de l'erreur 429 (Rate Limit Exceeded)\n",
    "                if \"429\" in str(e):\n",
    "                    # Calcul du temps d'attente : 2^attempt + aléatoire (Jitter)\n",
    "                    wait = 2 ** attempt + random.random()\n",
    "                    print(f\"Erreur 429… Retry dans {wait:.2f}s\")\n",
    "                    time.sleep(wait)\n",
    "                else:\n",
    "                    raise e\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304a8c2",
   "metadata": {},
   "source": [
    "3. Parcours du projet (itération sur fichiers)\n",
    "\n",
    "Cette fonction récupère seulement les fichiers code utiles, en respectant les exclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c196c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_project_files(folder_path, code_ext):\n",
    "    \"\"\"Itère uniquement sur les fichiers code utiles, en excluant dossiers et fichiers parasites.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        # On filtre les dossiers exclus\n",
    "        dirnames[:] = [d for d in dirnames if d not in excluded_dirs]\n",
    "\n",
    "        for f in filenames:\n",
    "            # Exclure les fichiers parasites\n",
    "            if f in excluded_files:\n",
    "                continue\n",
    "            # Exclure aussi par motif (ex: *.pyc, *.class, etc.)\n",
    "            for pattern in excluded_files:\n",
    "                if pattern.startswith(\"*.\") and f.endswith(pattern[1:]):\n",
    "                    break\n",
    "            else:\n",
    "                ext = os.path.splitext(f)[1].lower()\n",
    "                if ext in code_ext:\n",
    "                    yield os.path.join(dirpath, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621d0ef",
   "metadata": {},
   "source": [
    "## Pipeline de génération du README\n",
    "\n",
    "Nous avons décidé de suivre cette pipeline:\n",
    "1. Lecture et filtrage des fichiers code.\n",
    "2. Chunking et découpage.\n",
    "3. Création des embeddings.\n",
    "4. Indexation FAISS pour recherche par similarité.\n",
    "5. Construction du prompt RAG et génération du README via LLM.\n",
    "6. Sauvegarde du README dans le dossier du projet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6790f54",
   "metadata": {},
   "source": [
    "Le prompt RAG structure la génération pour garantir un README complet, clair et dans l’ordre :\n",
    "1. Titre du projet\n",
    "2. Présentation générale\n",
    "3. Arborescence\n",
    "4. Bibliothèques\n",
    "5. Résumé fichier par fichier\n",
    "6. Instructions pour lancer le projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49640cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme_RAG(folder_path, output_file):\n",
    "\n",
    "    # ---- 3.1 Lire fichiers code ----\n",
    "    code_ext = {\".py\", \".js\", \".ts\", \".cpp\", \".c\", \".java\", \".ipynb\", \".php\", \".html\"}\n",
    "    all_files = []\n",
    "    all_codes = []\n",
    "\n",
    "    for fp in iter_project_files(folder_path, code_ext):\n",
    "        code = read_code(fp)\n",
    "        if code.strip():\n",
    "            all_files.append(fp)\n",
    "            all_codes.append(code)\n",
    "\n",
    "    if not all_codes:\n",
    "        raise ValueError(\"Aucun fichier code trouvé dans ce dossier !\")\n",
    "\n",
    "  \n",
    "    # ---- 3.2 Chunking ----\n",
    "    chunks = []\n",
    "    chunk_paths = []\n",
    "\n",
    "    for file_path, code in zip(all_files, all_codes):\n",
    "        # Première étape : découper par fonctions / blocs\n",
    "        func_chunks = split_functions(code)\n",
    "\n",
    "        # Deuxième étape : si un chunk est trop long, on le re-découpe\n",
    "        for fc in func_chunks:\n",
    "            small_chunks = split_long_chunk(fc, max_chars=MAX_CHARS_PER_CHUNK)\n",
    "            for sc in small_chunks:\n",
    "                chunks.append(sc)\n",
    "                chunk_paths.append(file_path)\n",
    "\n",
    "    print(f\"Total chunks : {len(chunks)}\")\n",
    "\n",
    "\n",
    "    # ---- 3.3 Embeddings ----\n",
    "    # Vectorisation des chunks de code.\n",
    "    embeddings = create_embeddings(chunks, batch_size=16)\n",
    "    dim = len(embeddings[0]) # Dimension pour l'index FAISS\n",
    "\n",
    "    # ---- 3.4 Indexation FAISS ----\n",
    "    # Création d'un index vectoriel en mémoire pour la recherche rapide par similarité.\n",
    "    index = faiss.IndexFlatL2(dim) # Index FlatL2: distance euclidienne non compressée.\n",
    "    index.add(np.array(embeddings)) # Indexation/Stockage des vecteurs.\n",
    "\n",
    "    # ---- 3.5 Arborescence ----\n",
    "    # Génération de l'arborescence synthétique.\n",
    "    tree = show_tree(folder_path)\n",
    "\n",
    "    # ---- 3.6 Construire le prompt RAG final ----\n",
    "    # Création du prompt structuré pour guider le LLM (Codestral)\n",
    "    prompt = f\"\"\"\n",
    "Tu es un expert en analyse de code. \n",
    "Voici une liste de chunks extraits du projet.\n",
    "\n",
    "Ton rôle :\n",
    "- Reconstituer le sens du projet\n",
    "- Faire un README.md professionnel et intuitif\n",
    "\n",
    "Tu dois au maximum respecter exactement la structure suivante,\n",
    "avec ces titres dans cet ordre, même si tu n'as pas toutes les informations :\n",
    "1. # Titre du projet\n",
    "2. # Présentation générale et fonctionnement\n",
    "3. # Arborescence du projet\n",
    "4. # Bibliothèques nécessaires pour le projet\n",
    "5. # Résumé fichier par fichier\n",
    "6. # Comment lancer le projet\n",
    "\n",
    "Si tu manques d'informations pour une section, écris clairement :\n",
    "\"Informations à compléter\" plutôt que de supprimer la section.\n",
    "\n",
    "Voici l'arborescence du dossier :\n",
    "-----------------\n",
    "{tree}\n",
    "-----------------\n",
    "\n",
    "Voici un échantillon de chunks utiles (non ordonnés) :\n",
    "-----------------\n",
    "{chunks[:200]}\n",
    "-----------------\n",
    "\n",
    "\n",
    "Pour 2. # Présentation générale et fonctionnement en 20-30 lignes maximum\n",
    "Pour 3. # Arborescence du projet, si elle est trop longue, réduis-la en ne gardant que les dossiers et fichiers importants.\n",
    "Pour 5. # Résumé fichier par fichier, classe les par catégories de languages si possible, et pour chaque fichier, indique en 2-3 lignes son rôle dans le projet.\n",
    "pOUR 6. # Comment lancer le projet, indique les étapes précises pour un utilisateur lambda.\n",
    "Conclus le README par les avertissements sur le projet, et les limitations connues, en 5-10 lignes maximum.\n",
    "\n",
    "IMPORTANT : \n",
    "- Ne mets **aucun bloc de code Markdown** autour du README.\n",
    "- Ne commence PAS par ```markdown, ```md ou tout autre fence.\n",
    "\n",
    "Génère maintenant un README Markdown complet et propre, en respectant EXACTEMENT les titres et l'ordre des sections listées plus haut.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    # Appel au LLM (Codestral) pour la génération finale du README\n",
    "    response = client.chat.complete(\n",
    "        model=\"codestral-2508\", \n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2, # Température basse pour une réponse factuelle et stable\n",
    "        max_tokens=5000 \n",
    "    )\n",
    "\n",
    "    readme_text = response.choices[0].message.content\n",
    "\n",
    "    # ---- 3.7 Sauvegarde ----\n",
    "    # Si README.md existe, on crée README2.md\n",
    "    if os.path.exists(output_file):\n",
    "        base = os.path.dirname(output_file)\n",
    "        output_file = os.path.join(base, \"README2.md\")\n",
    "        print(\"README.md existant → génération dans README2.md\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme_text)\n",
    "\n",
    "    print(\"\\n README généré :\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f04bc",
   "metadata": {},
   "source": [
    "## Création du README de l'utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14c23760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks : 26\n",
      "Erreur 429… Retry dans 1.08s\n",
      "Erreur 429… Retry dans 2.26s\n",
      "\n",
      " README généré : D:\\33611\\Documents\\MASTER\\M2\\PROJET_README\\creating-a-README-for-a-codebase\\README.md\n"
     ]
    }
   ],
   "source": [
    "# --- Déclenchement du pipeline de génération ---\n",
    "# Demande à l'utilisateur de spécifier le chemin du dossier du projet.\n",
    "folder = input(\"Dossier à analyser : \").strip()\n",
    "\n",
    "# Définition du chemin de sortie par défaut\n",
    "output = os.path.join(folder, \"README.md\")\n",
    "\n",
    "# Lancement du processus RAG + génération README\n",
    "generate_readme_RAG(folder, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe4c25",
   "metadata": {},
   "source": [
    "## Guide Utilisateur et Avertissements\n",
    "\n",
    "Cette partie sert à guider l’utilisateur qui veut générer automatiquement un README pour son projet en utilisant ce notebook.\n",
    "\n",
    "### Guide d’Exécution\n",
    "\n",
    "Pour faire fonctionner la génération du README, voici les étapes à suivre :\n",
    "\n",
    "1. Préparer l’Environnement :\n",
    "    - Vérifiez que votre clé API Mistral (MISTRAL_API_KEY) est correctement configurée dans le fichier .env.\n",
    "    - Assurez-vous que toutes les bibliothèques nécessaires sont installées, par exemple : numpy, mistralai, faiss-cpu, etc.\n",
    "\n",
    "2. Lancer le Notebook :\n",
    "    - Exécutez toutes les cellules du notebook dans l’ordre, de la première (imports) jusqu’à la dernière cellule de génération.\n",
    "\n",
    "3. Indiquer le Projet à Analyser :\n",
    "    - Lorsque le notebook demande “Dossier à analyser”, copiez-collez le chemin complet du dossier racine de votre projet.\n",
    "      - Exemple :\n",
    "          - Windows : C:\\Users\\NomUtilisateur\\MonProjet\n",
    "          - Mac/Linux : ~/Documents/MonProjet\n",
    "\n",
    "4. Suivre l’Exécution :\n",
    "Si vous voyez des erreurs 429, pas de panique !\n",
    "Ce sont des limites d’API (trop de requêtes trop rapides).\n",
    "Le système attend quelques secondes et réessaye automatiquement grâce au mécanisme de backoff exponentiel avec Jitter.\n",
    "\n",
    "Vous verrez alors des messages du type :\n",
    "\n",
    "\"Total chunks : 26\n",
    "Erreur 429… Retry dans 1.08s\n",
    "Erreur 429… Retry dans 2.26s\"\n",
    "\n",
    "Tout est normal, l’exécution continue toute seule.\n",
    "\n",
    "5. Récupérer le Résultat :\n",
    "\n",
    "   - Une fois terminé, un message indique : \"README généré : [chemin/vers/README.md]\"\n",
    "\n",
    "  - Le fichier README sera créé dans le dossier racine du projet.\n",
    "      - Si un README existait déjà, le nouveau sera nommé README2.md.\n",
    "\n",
    "### Avertissements et Limites Connues\n",
    "\n",
    "Quelques points importants à connaître pour utiliser ce système correctement :\n",
    "\n",
    "1. Qualité du Code Source :\n",
    "    - Plus votre code est clair et bien structuré (variables explicites, fonctions bien nommées…), plus le README généré sera précis et utile.\n",
    "    - Un code confus ou mal organisé donnera un README moins pertinent.\n",
    "\n",
    "2. Types de Fichiers :\n",
    "   - Le système est optimisé pour le code source (Python, JS, C++, etc.).\n",
    "   - Certains fichiers spécifiques ou binaires (images, fichiers compilés…) ne seront pas analysés correctement.\n",
    "\n",
    "1. Performance et Scalabilité :\n",
    "   - Le temps de génération dépend de la taille du projet.\n",
    "   - Pour des projets très volumineux (des milliers de fichiers), l’index FAISS en mémoire peut devenir un goulot d’étranglement, et la qualité des résultats peut diminuer.\n",
    "   - Dans ces cas, il peut être nécessaire de diviser le projet en sous-dossiers ou de générer le README par parties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc7eac",
   "metadata": {},
   "source": [
    "## Limites liées à la Parallélisation\n",
    "\n",
    "Dans ce projet, certaines étapes comme le chunking et la génération des embeddings vectoriels pourraient bénéficier d’une parallélisation afin d’accélérer le traitement. Cependant, plusieurs contraintes limitent l’efficacité de cette approche :\n",
    "\n",
    "- Séquentialité obligatoire pour certaines étapes :\n",
    "\n",
    "    L’indexation FAISS et la génération finale du README reposent sur une structure séquentielle. Tenter de paralléliser ces étapes pourrait entraîner des conflits et des incohérences dans l’index ou le résultat final.\n",
    "\n",
    "- Coût mémoire et communication :\n",
    "\n",
    "    La parallélisation locale augmente la consommation mémoire et nécessite une communication inter-processus plus complexe, ce qui peut contrebalancer les gains de performance.\n",
    "\n",
    "- Granularité limitée :\n",
    "\n",
    "    Pour de petits projets ou des chunks déjà optimisés, le bénéfice réel de la parallélisation est faible. En revanche, pour des projets très volumineux, des solutions plus sophistiquées (indexation cloud, traitement hiérarchique) peuvent offrir un vrai gain de temps.\n",
    "\n",
    "En résumé, la parallélisation peut aider, mais elle doit être appliquée avec précaution et est surtout pertinente pour des projets de grande taille ou dans des environnements distribués."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4feb03e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce projet avait pour objectif de générer automatiquement un README à partir d’un projet de code source en utilisant une architecture RAG (Retrieval-Augmented Generation). L’intégration du parsing du code, des embeddings vectoriels et d’un LLM a permis de produire une documentation structurée, cohérente et compréhensible, même lorsque le code source était peu commenté.\n",
    "\n",
    "1. Les points forts du système sont :\n",
    "   - Chunking optimisé : les fonctions et blocs de code sont découpés intelligemment pour respecter les contraintes des API et faciliter l’indexation\n",
    "   - Filtrage efficace des fichiers : les fichiers inutiles ou non pertinents sont exclus, ce qui améliore la qualité globale du README\n",
    "   - Prompt RAG structuré : la génération finale suit un format précis, garantissant la cohérence et la lisibilité de la documentation.\n",
    "\n",
    "2. Cependant, certaines limites persistent :\n",
    "    - La scalabilité reste un défi pour les très grands projets.\n",
    "    - La mémoire nécessaire pour l’index FAISS peut devenir un goulot d’étranglement.\n",
    "    - La fenêtre contextuelle des modèles limite la quantité de code pouvant être analysée simultanément.\n",
    "    - La parallélisation locale présente des gains limités et doit être utilisée avec prudence.\n",
    "\n",
    "En conclusion, ce travail démontre que la RAG est une approche efficace pour automatiser la documentation logicielle, offrant un cadre solide pour générer des README lisibles et structurés, et ouvre la voie à des améliorations futures adaptées aux projets volumineux et complexes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
