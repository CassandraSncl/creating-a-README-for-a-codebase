{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0acc9c",
   "metadata": {},
   "source": [
    "# Assistant de Recherche pour la Génération Automatique de README"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e2a72",
   "metadata": {},
   "source": [
    "## 1. INTRODUCTION\n",
    "\n",
    "Dans le monde du développement logiciel, la documentation est souvent reléguée au second plan, malgré son rôle central. Un README clair et complet est pourtant la première impression qu’un utilisateur ou un contributeur aura d’un projet : il facilite la prise en main, l’installation et la compréhension des fonctionnalités. Pourtant, écrire et maintenir une documentation à jour est long, fastidieux et facilement négligé, surtout dans des projets en constante évolution.\n",
    "\n",
    "Ce projet propose une solution révolutionnaire pour tous les développeurs : un assistant automatique capable d’analyser un projet complet et de générer un README.md structuré, cohérent et fidèle à l’état actuel du code.\n",
    "\n",
    "Les bénéfices sont immédiats et concrets :\n",
    "- Gain de temps massif : fini les heures passées à rédiger manuellement un README pour chaque projet ou chaque mise à jour.\n",
    "- Documentation toujours à jour : en analysant directement le code source, l’outil élimine le phénomène de \"documentation drift\" où les README deviennent obsolètes par rapport au code réel.\n",
    "- Accessibilité et collaboration : un README clair permet à n’importe quel collaborateur ou utilisateur de comprendre et de contribuer rapidement au projet.\n",
    "- Polyvalence : même les projets multi-langages ou peu commentés peuvent bénéficier d’une documentation complète et professionnelle.\n",
    "\n",
    "En automatisant ce processus, ce projet ne se contente pas de simplifier la vie des développeurs : il change leur manière de travailler, en rendant la documentation plus rapide, fiable et accessible. \n",
    "\n",
    "Imaginez des milliers de projets à travers le monde, chacun avec un README précis et clair, généré automatiquement. Ce projet a le potentiel de transformer la manière dont les développeurs documentent leurs projets, tout en améliorant la collaboration et la maintenance logicielle à grande échelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327bb5d",
   "metadata": {},
   "source": [
    "# Présentation du Sujet\n",
    "\n",
    "Le projet consiste en la création d'un **assistant intelligent d'analyse de code** capable de :\n",
    "\n",
    "1. **Analyser automatiquement** un dossier de projet contenant du code source\n",
    "2. **Déduire la structure** et l'organisation du projet\n",
    "3. **Générer un fichier README.md** complet et professionnel\n",
    "\n",
    "## 1.1 Contenu du README Généré\n",
    "\n",
    "Le fichier README généré doit obligatoirement contenir :\n",
    "\n",
    "- Un **résumé clair** du projet et de son fonctionnement\n",
    "- Une **arborescence détaillée** de la structure du dossier\n",
    "- Les **bibliothèques et dépendances** nécessaires\n",
    "- Un **résumé fichier par fichier** du code\n",
    "- Les **instructions de lancement** du projet\n",
    "- Les **limitations et avertissements** éventuels\n",
    "\n",
    "Le défi principal réside dans la compréhension sémantique du code : il ne s'agit pas simplement de lister des fichiers, mais de comprendre leur rôle, leurs interactions et l'architecture globale du projet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc879f11",
   "metadata": {},
   "source": [
    "## 1.2 Intérêt du Projet\n",
    "\n",
    "Ce projet s'inscrit dans une problématique d'automatisation intelligente de la documentation. L'objectif est de combiner les techniques de traitement du langage naturel (NLP), l'architecture RAG (Retrieval-Augmented Generation) et les capacités des modèles de langage pour créer un assistant capable d'analyser automatiquement un projet et de générer une documentation structurée et cohérente. \n",
    "\n",
    "Dans le cadre du cours de Programmation Avancée et Calcul Parallèle, ce projet permet d'explorer des concepts avancés tels que :\n",
    "\n",
    "- **Standardisation** : garantir une structure homogène de documentation\n",
    "- **Analyse sémantique** : comprendre le code au-delà de sa syntaxe\n",
    "- **Scalabilité** : traiter des projets de tailles variables grâce au chunking intelligent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c891a59d",
   "metadata": {},
   "source": [
    "# 2. Pipeline de Développement\n",
    "\n",
    "### Étape 1 : Extraction de l'Infrastructure\n",
    "\n",
    "À partir du dossier racine, parcourir récursivement l'arborescence pour identifier tous les fichiers et sous-dossiers. Filtrer intelligemment les éléments parasites.\n",
    "\n",
    "### Étape 2 : Séparation Code vs Documentation\n",
    "\n",
    "Distinguer les fichiers contenant du code exécutable (`.py`, `.js`, `.cpp`, `.ipynb`, etc.) des fichiers de documentation ou de configuration. Cette étape permet de concentrer l'analyse sur le code fonctionnel.\n",
    "\n",
    "### Étape 3 : Embedding et Chunking avec RAG\n",
    "\n",
    "Cette étape constitue le cœur technique du projet :\n",
    "\n",
    "#### 3.1. Chunking intelligent\n",
    "\n",
    "- Découper le code en blocs cohérents (fonctions, classes, modules)\n",
    "- Gérer les chunks trop longs en les subdivisant (limite : 5000 caractères)\n",
    "- Préserver la cohérence sémantique lors du découpage\n",
    "\n",
    "#### 3.2. Génération des embeddings\n",
    "\n",
    "- Transformer chaque chunk en vecteur numérique via l'API Mistral (`codestral-embed`)\n",
    "- Utiliser des embeddings spécialisés pour le code\n",
    "\n",
    "#### 3.3. Indexation vectorielle\n",
    "\n",
    "- Stocker les vecteurs dans un index FAISS (Facebook AI Similarity Search)\n",
    "- Permettre une recherche sémantique rapide et efficace\n",
    "\n",
    "\n",
    "#### 3.4. Analyse Sémantique via LLM\n",
    "\n",
    "Utiliser un modèle de langage (`codestral-2508`) pour interpréter le rôle et la logique des fonctions extraites.\n",
    "\n",
    "### Étape 4 : Récupération des Prérequis\n",
    "\n",
    "Extraire automatiquement les imports et dépendances pour reconstituer la liste des bibliothèques nécessaires au projet.\n",
    "\n",
    "### Étape 5 : Génération du README\n",
    "\n",
    "Synthétiser toutes les informations collectées dans un prompt structuré et générer un README.md cohérent via le LLM, en respectant une structure prédéfinie stricte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6face",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdafeb17",
   "metadata": {},
   "source": [
    "### Paramétrage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5eefbf",
   "metadata": {},
   "source": [
    "1. Importation des bibliothèques nécessaires :\n",
    "   \n",
    "On commence par importer toutes les bibliothèques utiles pour le projet. On ajoute notamment `concurrent.futures` pour la **parallélisation** des tâches indépendantes, cruciale pour l'optimisation des performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "37af9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires pour le projet.\n",
    "\n",
    "import os            # Pour les opérations système (gestion des chemins, etc.)\n",
    "import re            # Pour les expressions régulières (utilisé pour le chunking)\n",
    "import json          # Pour la gestion des fichiers JSON (notamment les .ipynb)\n",
    "import numpy as np   # Pour les opérations numériques (essentiel pour les embeddings)\n",
    "from dotenv import load_dotenv # Pour charger la clé API depuis un fichier .env\n",
    "from mistralai import Mistral # SDK officiel pour interagir avec Mistral AI\n",
    "import faiss         # Bibliothèque d'indexation vectorielle (recherche d'embeddings)\n",
    "import time          # Pour la gestion des pauses (retry en cas d'erreur 429)\n",
    "import random        # Pour la gestion des pauses aléatoires (backoff jitter)\n",
    "import concurrent.futures # Pour la parallélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e234510",
   "metadata": {},
   "source": [
    "2. Récupération de la clé API pour l'appel à Miastral AI.\n",
    "\n",
    "Cette étape est cruciale pour sécuriser la clé API et permettre l’accès aux services Mistral AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "76449ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe24625",
   "metadata": {},
   "source": [
    "3. Exclusion de fichiers/dossiers inutiles\n",
    "\n",
    "Le pipeline n’analyse que le code pertinent. On exclut donc :\n",
    "- Les environnements virtuels, caches, artefacts de build, fichiers binaires.\n",
    "- Les fichiers temporaires et images inutiles à l’analyse.\n",
    "\n",
    "Cela améliore les performances et réduit le nombre de chunks inutiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d1562080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des dossiers à exclure (Python, Node.js, Java, etc.)\n",
    "excluded_dirs = {\n",
    "    # Python\n",
    "    \"venv\", \".venv\", \"__pycache__\", \"site-packages\", \"env\", \".env\",\n",
    "    # Node.js / JS\n",
    "    \"node_modules\", \"bower_components\", \".npm\", \".yarn\",\n",
    "    # Java / JVM\n",
    "    \"target\", \"build\", \".gradle\", \".mvn\", \"out\",\n",
    "    # C/C++ / Rust / Go / .NET\n",
    "    \"cmake-build-debug\", \"cmake-build-release\", \"bin\", \"obj\", \"pkg\", \"dist\",\n",
    "    \"Debug\", \"Release\",\n",
    "    # Divers IDE / SCM\n",
    "    \".git\", \".svn\", \".hg\", \".idea\", \".vscode\"\n",
    "}\n",
    "\n",
    "# Liste des fichiers à exclure (souvent générés automatiquement ou inutiles à l'analyse)\n",
    "excluded_files = {\n",
    "    # SCM / VCS\n",
    "    \".gitignore\", \".gitattributes\", \".gitmodules\",\n",
    "    \".hgignore\", \".svnignore\",\n",
    "\n",
    "    # Config / lockfiles\n",
    "    \"package-lock.json\", \"yarn.lock\", \"pnpm-lock.yaml\",\n",
    "    \"poetry.lock\", \"Pipfile.lock\",\n",
    "\n",
    "    # Build / cache\n",
    "    \"Thumbs.db\", \"Desktop.ini\",\n",
    "    \".DS_Store\",  # macOS\n",
    "    \"npm-debug.log\", \"yarn-error.log\",\n",
    "    \"Cargo.lock\", \"Gemfile.lock\",\n",
    "\n",
    "    # Environnements\n",
    "    \".env\", \".env.local\", \".env.production\", \".env.development\",\n",
    "\n",
    "    # Binaires / artefacts\n",
    "    \"*.pyc\", \"*.pyo\", \"*.pyd\",\n",
    "    \"*.class\", \"*.jar\", \"*.war\", \"*.ear\",\n",
    "    \"*.dll\", \"*.so\", \"*.dylib\",\n",
    "    \"*.exe\", \"*.out\", \"*.o\", \"*.obj\",\n",
    "    \"*.a\", \"*.lib\",\n",
    "\n",
    "    # Archives\n",
    "    \"*.zip\", \"*.tar\", \"*.gz\", \"*.bz2\", \"*.rar\",\n",
    "\n",
    "    # Divers\n",
    "    \"README.md\", \"LICENSE\", \"COPYING\", \"CHANGELOG\", \"TODO\", \"Makefile\",\n",
    "    \n",
    "    # Photos \n",
    "    \"*.png\", \"*.jpg\", \"*.jpeg\", \"*.gif\", \"*.bmp\", \"*.svg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b244c8",
   "metadata": {},
   "source": [
    "## Fonctions principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbec4cb",
   "metadata": {},
   "source": [
    "1. Affichage de l’arborescence (show_tree)\n",
    "\n",
    "Cette fonction affiche l’arborescence du projet jusqu’au second niveau seulement. Elle n'affiche pas les images également. \n",
    "\n",
    "Cela évite d’afficher des milliers de fichiers pour les gros projets. Et donc de se limiter au premier et second niveaux dans l'arborescence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a5f809a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tree(root, code_ext=None):\n",
    "    \"\"\"\n",
    "    Renvoie une arborescence synthétique du dossier en ne gardant que les fichiers de code.\n",
    "    code_ext : set d'extensions à inclure, ex: {'.py', '.js', '.ipynb'}\n",
    "    \"\"\"\n",
    "    if code_ext is None:\n",
    "        code_ext = {\".py\", \".js\", \".ts\", \".cpp\", \".c\", \".java\", \".ipynb\", \".php\", \".html\"}\n",
    "\n",
    "    lines = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        # filtrer les fichiers qui ne sont pas du code\n",
    "        code_files = [f for f in filenames if os.path.splitext(f)[1].lower() in code_ext]\n",
    "        if code_files:\n",
    "            level = dirpath.replace(root, \"\").count(os.sep)\n",
    "            indent = \" \" * 4 * level\n",
    "            lines.append(f\"{indent}{os.path.basename(dirpath)}/\")\n",
    "            subindent = \" \" * 4 * (level + 1)\n",
    "            for f in code_files:\n",
    "                lines.append(f\"{subindent}{f}\")\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34440f6",
   "metadata": {},
   "source": [
    "2. Lecture du code (read_code)\n",
    "\n",
    "Cette fonction lit uniquement le code exploitable :\n",
    "- Pour les .ipynb, seules les cellules de type \"code\" sont conservées.\n",
    "- Pour les autres fichiers, tout le contenu est récupéré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "888448b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_code(file_path):\n",
    "    \"\"\"\n",
    "    Lit le contenu d'un fichier de code et renvoie uniquement le code exploitable.\n",
    "\n",
    "    - Pour les fichiers Jupyter Notebook (.ipynb) : \n",
    "      ne conserve que le contenu des cellules de type 'code', et les concatène en une seule chaîne de caractères.\n",
    "    - Pour les autres fichiers de code (.py, .js, .cpp, etc.) :\n",
    "      lit l'intégralité du fichier et retourne son contenu brut.\n",
    "\n",
    "    Paramètre :\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Chemin vers le fichier à lire.\n",
    "\n",
    "    Retour :\n",
    "    -------\n",
    "    str\n",
    "        Contenu du code du fichier, prêt à être utilisé pour le traitement ou\n",
    "        pour la génération de chunks dans un pipeline de documentation.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if ext == \".ipynb\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            nb = json.load(f)\n",
    "            content = []\n",
    "            for cell in nb.get(\"cells\", []):\n",
    "                if cell.get(\"cell_type\") == \"code\":\n",
    "                    content.append(\"\".join(cell.get(\"source\", [])))\n",
    "            return \"\\n\".join(content)\n",
    "\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00662dc",
   "metadata": {},
   "source": [
    "## Découpage du code en chunks + embedding\n",
    "\n",
    "**Optimisation :** Cette phase est désormais traitée en **parallèle** pour réduire le temps de lecture des fichiers et le découpage des fonctions.\n",
    "\n",
    "1. Découpage du code en chunks\n",
    "\n",
    "Nous avons donc deux fonctions qui travaillent ensemble :\n",
    "- split_functions : découpe le code en fonctions ou blocs logiques.\n",
    "- split_long_chunk : si un chunk est trop long, on le découpe en morceaux plus petits.\n",
    "\n",
    "Cela permet d'assurer que chaque chunk respecte la limite maximale pour l’API d’embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2edc8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_functions(code):\n",
    "    \"\"\"Découpe le code en fonctions/chunks (première étape de chunking logique).\"\"\"\n",
    "    # Regex : recherche de motifs de fonctions 'def nom(...):' et capture leur corps.\n",
    "    # re.DOTALL (ou S) est crucial pour que le '.' matche les sauts de ligne à l'intérieur de la fonction.\n",
    "    pattern = r\"(def [\\w_]+\\s*\\([^)]*\\):(?:\\n(?:\\s+.+))*)\" \n",
    "    chunks = re.findall(pattern, code, re.DOTALL)\n",
    "    # Si aucune fonction (ex: script simple ou code dans .ipynb), le chunk est le fichier entier.\n",
    "    return chunks if chunks else [code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4a10f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille max en caractères pour un chunk envoyé aux embeddings\n",
    "MAX_CHARS_PER_CHUNK = 5000\n",
    "\n",
    "def split_long_chunk(chunk, max_chars=MAX_CHARS_PER_CHUNK):\n",
    "    \"\"\"\n",
    "    Si un chunk est trop long, on le découpe en morceaux plus petits basés sur les lignes.\n",
    "    On essaye de garder des morceaux cohérents tout en respectant la limite de taille.\n",
    "    \"\"\"\n",
    "    if len(chunk) <= max_chars:\n",
    "        return [chunk]\n",
    "\n",
    "    lines = chunk.splitlines()\n",
    "    parts = []\n",
    "    current = []\n",
    "    current_len = 0\n",
    "\n",
    "    for line in lines:\n",
    "        # +1 pour le saut de ligne\n",
    "        extra = len(line) + 1\n",
    "        # Si on dépasse la limite, on coupe ici\n",
    "        if current_len + extra > max_chars and current:\n",
    "            parts.append(\"\\n\".join(current))\n",
    "            current = [line]\n",
    "            current_len = extra\n",
    "        else:\n",
    "            current.append(line)\n",
    "            current_len += extra\n",
    "\n",
    "    if current:\n",
    "        parts.append(\"\\n\".join(current))\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3325f7",
   "metadata": {},
   "source": [
    "2. Création des embeddings (Parallélisée)\n",
    "\n",
    "Chaque chunk est transformé en vecteur numérique grâce au modèle `codestral-embed`.\n",
    "\n",
    "**Parallélisation :** La fonction `create_embeddings_parallélisé` utilise désormais un `ThreadPoolExecutor` pour envoyer plusieurs lots (`batches`) à l'API Mistral **simultanément**. Ceci permet de maximiser le débit et de contourner plus efficacement les limites de requêtes par seconde (`Rate Limit - 429`) en utilisant un mécanisme de **backoff exponentiel avec Jitter** pour chaque appel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9afe115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille max en caractères pour un chunk envoyé aux embeddings\n",
    "MAX_CHARS_PER_CHUNK = 5000 \n",
    "# Nombre maximal de lots (batches) à envoyer simultanément à l'API.\n",
    "MAX_CONCURRENT_BATCHES = 4 # Ajustez selon la limite de votre clé API\n",
    "\n",
    "def _get_embedding_batch(client, batch, max_retries):\n",
    "    \"\"\"Fonction utilitaire pour créer un seul lot d'embeddings avec retry.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Appel à l'API Mistral AI\n",
    "            resp = client.embeddings.create( \n",
    "                model=\"codestral-embed\",\n",
    "                inputs=batch\n",
    "            )\n",
    "            # Retourne les embeddings du lot\n",
    "            return [np.array(emb.embedding, dtype=np.float32) for emb in resp.data] \n",
    "\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                wait = 2 ** attempt + random.random()\n",
    "                print(f\"Erreur 429... Retry dans {wait:.2f}s pour un lot d'embeddings.\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                raise e\n",
    "    raise Exception(\"Échec de la génération des embeddings après tous les essais.\")\n",
    "\n",
    "\n",
    "def create_embeddings_parallélisé(chunks, batch_size=16, max_retries=5):\n",
    "    \"\"\"\n",
    "    Génère les vecteurs d'embeddings en parallèle en envoyant plusieurs lots (batches) \n",
    "    simultanément à l'API Mistral AI.\n",
    "    \"\"\"\n",
    "    all_batches = []\n",
    "    # Création de tous les lots (batches)\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        all_batches.append(chunks[i:i+batch_size])\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    # Utilisation du ThreadPoolExecutor pour l'appel API (I/O-bound)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_CONCURRENT_BATCHES) as executor:\n",
    "        \n",
    "        # On soumet chaque lot (batch) à l'exécution asynchrone\n",
    "        # On utilise un \"lambda\" pour passer des arguments fixes (client, max_retries)\n",
    "        futures = [executor.submit(_get_embedding_batch, client, batch, max_retries) \n",
    "                   for batch in all_batches]\n",
    "\n",
    "        # On récupère les résultats au fur et à mesure que les futures sont terminées\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                # La fonction retourne une liste d'embeddings pour ce lot\n",
    "                batch_embeddings = future.result() \n",
    "                embeddings.extend(batch_embeddings)\n",
    "            except Exception as e:\n",
    "                # Gérer les exceptions non traitées dans _get_embedding_batch\n",
    "                print(f\"Une erreur s'est produite lors de la récupération d'un lot : {e}\")\n",
    "                # Vous pourriez choisir d'ignorer ou de relancer ici\n",
    "                \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304a8c2",
   "metadata": {},
   "source": [
    "3. Parcours du projet (itération sur fichiers)\n",
    "\n",
    "Cette fonction récupère seulement les fichiers code utiles, en respectant les exclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c196c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_project_files(folder_path, code_ext):\n",
    "    \"\"\"Itère uniquement sur les fichiers code utiles, en excluant dossiers et fichiers parasites.\"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        # On filtre les dossiers exclus\n",
    "        dirnames[:] = [d for d in dirnames if d not in excluded_dirs]\n",
    "\n",
    "        for f in filenames:\n",
    "            # Exclure les fichiers parasites\n",
    "            if f in excluded_files:\n",
    "                continue\n",
    "            # Exclure aussi par motif (ex: *.pyc, *.class, etc.)\n",
    "            for pattern in excluded_files:\n",
    "                if pattern.startswith(\"*.\") and f.endswith(pattern[1:]):\n",
    "                    break\n",
    "            else:\n",
    "                ext = os.path.splitext(f)[1].lower()\n",
    "                if ext in code_ext:\n",
    "                    yield os.path.join(dirpath, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2101495",
   "metadata": {},
   "source": [
    "4. Fonction utilitaire de traitement par fichier (`process_file_for_chunking`)\n",
    "\n",
    "Cette fonction enveloppe la lecture et les deux étapes de découpage (`split_functions` et `split_long_chunk`). Elle est essentielle car c'est cette unité de travail qui sera **exécutée en parallèle** par le `ThreadPoolExecutor` sur chaque fichier du projet. Elle permet de lier immédiatement chaque chunk au chemin de son fichier d'origine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "3d7aced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction utilitaire pour le mapping\n",
    "def process_file_for_chunking(file_path, code_ext):\n",
    "    \"\"\"\n",
    "    Lit un fichier, génère les chunks et retourne une liste de (chunk, path).\n",
    "    Fonction adaptée pour être mappée par un Executor.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext not in code_ext:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        code = read_code(file_path)\n",
    "        if not code.strip():\n",
    "            return []\n",
    "\n",
    "        # Première étape : découper par fonctions / blocs\n",
    "        func_chunks = split_functions(code)\n",
    "\n",
    "        results = []\n",
    "        # Deuxième étape : si un chunk est trop long, on le re-découpe\n",
    "        for fc in func_chunks:\n",
    "            small_chunks = split_long_chunk(fc, max_chars=MAX_CHARS_PER_CHUNK)\n",
    "            for sc in small_chunks:\n",
    "                results.append((sc, file_path))\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur de traitement du fichier {file_path}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621d0ef",
   "metadata": {},
   "source": [
    "## Pipeline de génération du README\n",
    "\n",
    "Le pipeline suit la logique RAG (Retrieval-Augmented Generation) optimisée :\n",
    "1. Lecture et filtrage des fichiers code **(Parallélisé)**.\n",
    "2. Chunking et découpage **(Parallélisé)**.\n",
    "3. Création des embeddings **(Parallélisé)**.\n",
    "4. Indexation FAISS pour recherche par similarité **(Séquentiel)**.\n",
    "5. **Construction optimisée du prompt RAG (avec chemins de fichiers)** et génération du README via LLM.\n",
    "6. Sauvegarde du README dans le dossier du projet.\n",
    "\n",
    "L'utilisation de la parallélisation permet un gain de temps significatif sur les projets volumineux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6790f54",
   "metadata": {},
   "source": [
    "Le prompt RAG structure la génération pour garantir un README complet, clair et dans l’ordre :\n",
    "1. Titre du projet\n",
    "2. Présentation générale\n",
    "3. Arborescence\n",
    "4. Bibliothèques\n",
    "5. Résumé fichier par fichier\n",
    "6. Instructions pour lancer le projet\n",
    "7. Avertissements et Limitations (Nouvelle section finale)\n",
    "\n",
    "**Amélioration du contexte RAG :** Le bloc de chunks envoyé au LLM est désormais **enrichi du nom de fichier d'origine** pour chaque morceau de code. Cette information est cruciale pour que le modèle puisse rédiger correctement la section \"Résumé fichier par fichier\" avec précision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "49640cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:132: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:132: SyntaxWarning: invalid escape sequence '\\`'\n",
      "C:\\Users\\33611\\AppData\\Local\\Temp\\ipykernel_23120\\3183622200.py:132: SyntaxWarning: invalid escape sequence '\\`'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def generate_readme_RAG(folder_path, output_file, max_workers=4): # max_workers est le paramètre de parallélisation\n",
    "    \n",
    "    # ---- 3.1 Lire et Chunquer les fichiers en parallèle ----\n",
    "    code_ext = {\".py\", \".js\", \".ts\", \".cpp\", \".c\", \".java\", \".ipynb\", \".php\", \".html\"}\n",
    "    all_files_to_process = list(iter_project_files(folder_path, code_ext))\n",
    "    \n",
    "    chunks = []\n",
    "    chunk_paths = []\n",
    "\n",
    "    # Utilisation du ThreadPoolExecutor pour le parallélisme I/O et CPU léger\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # On soumet chaque fichier à la fonction process_file_for_chunking\n",
    "        # executor.map applique la fonction à chaque élément de la liste\n",
    "        futures = [executor.submit(process_file_for_chunking, fp, code_ext) \n",
    "                   for fp in all_files_to_process]\n",
    "        \n",
    "        # On récupère les résultats au fur et à mesure\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            results = future.result()\n",
    "            for chunk, file_path in results:\n",
    "                chunks.append(chunk)\n",
    "                chunk_paths.append(file_path)\n",
    "\n",
    "\n",
    "    if not chunks:\n",
    "        raise ValueError(\"Aucun code utile trouvé dans ce dossier après chunking !\")\n",
    "\n",
    "    print(f\"Total chunks : {len(chunks)}\")\n",
    "\n",
    "\n",
    "    # ---- 3.3 Embeddings (PARALLÉLISÉ) ----\n",
    "    print(\"Génération des embeddings en parallèle...\")\n",
    "    embeddings = create_embeddings_parallélisé(chunks, batch_size=16)\n",
    "    dim = len(embeddings[0]) \n",
    "\n",
    "    # ---- 3.4 Indexation FAISS (Séquentiel obligatoire) ----\n",
    "    # Cette étape DOIT rester séquentielle pour garantir l'intégrité de l'index\n",
    "    print(\"Indexation FAISS...\")\n",
    "    index = faiss.IndexFlatL2(dim) \n",
    "    index.add(np.array(embeddings)) # Indexation/Stockage des vecteurs.\n",
    "\n",
    "    # ---- 3.5 Arborescence ----\n",
    "    # Génération de l'arborescence synthétique.\n",
    "    tree = show_tree(folder_path)\n",
    "\n",
    "    # ---- 3.6 Construire le prompt RAG final ----\n",
    "    # Création du prompt structuré pour guider le LLM (Codestral)\n",
    "    prompt = f\"\"\"\n",
    "    Tu es **un expert en analyse de code et rédaction technique**.  \n",
    "    Tu dois analyser un projet à partir d’une **liste de chunks de code non ordonnés** et **l’arborescence du projet**.\n",
    "\n",
    "    Ton objectif : Produire un `README.md` **professionnel, lisible et structuré**, destiné à des développeurs ou utilisateurs techniques.\n",
    "\n",
    "    ----------------------------------------\n",
    "    ### Contraintes obligatoires\n",
    "\n",
    "    Respecte EXACTEMENT l’ordre et les titres suivants :\n",
    "\n",
    "    1. # Titre du projet  \n",
    "    2. # Présentation générale et fonctionnement  \n",
    "    3. # Arborescence du projet des fichiers importants  \n",
    "    4. # Bibliothèques nécessaires pour le projet  \n",
    "    5. # Résumé catégories par catégories  \n",
    "    6. # Comment lancer le projet  \n",
    "\n",
    "    Tu peux **ajouter d’autres sections facultatives** UNIQUEMENT si elles sont pertinentes (ex : # API, # Tests, # Cas d’usage, # Structure des données, # Limitations techniques).  \n",
    "    **Ces sections ne doivent pas faire dépasser les 200 lignes au total.**\n",
    "\n",
    "    Si une section est incomplète ou manque d’informations, tu dois écrire :\n",
    "    > Informations à compléter\n",
    "\n",
    "    **INTERDIT** :\n",
    "    - Pas de blocs de code Markdown autour du README\n",
    "    - Ne pas commencer par \\`\\`\\`markdown, \\`\\`\\`md, \\`\\`\\`bash…\n",
    "    - Ne pas réordonner les sections obligatoires\n",
    "    - Ne dépasse pas les 200 lignes au maximum dans le fichier readme final\n",
    "    - Ne fais pas d'indentation avec des espaces ou des tabulations dans les titres Markdown\n",
    "\n",
    "    ----------------------------------------\n",
    "    ### Instructions détaillées\n",
    "\n",
    "    #### Pour 2. # Présentation générale\n",
    "    - Ne dépasse pas 20–30 lignes\n",
    "    - Expliquer l’objectif, l’utilité et le fonctionnement global du projet\n",
    "    - Déduis le domaine si possible (IA, web, data, simulation, etc.)\n",
    "\n",
    "    #### Pour 3. # Arborescence\n",
    "    - Regarde l’arborescence donnée mais **ne l’affiche pas telle quelle**.\n",
    "    - Ne montrer que les **fichiers et dossiers de code importants** (scripts, modules principaux, routes, notebooks, etc.).\n",
    "    - Ignorer complètement les fichiers non-code comme images, fichiers temporaires, logs, docs inutiles, etc.\n",
    "    - Ne pas inventer de fichiers ou dossiers qui n’existent pas.\n",
    "    - Présenter l’arborescence de manière **synthétique et lisible** pour un développeur.\n",
    "\n",
    "\n",
    "    #### Pour 5. # Résumé catégories par catégories\n",
    "    - Classe si possible **par catégories (langage, modules, front/back, scripts, modèles, etc.)**\n",
    "    - Si on a un site web, de parler des routes\n",
    "\n",
    "    #### Pour 6. # Comment lancer le projet\n",
    "    C'est une partie cruciale du README :\n",
    "    - Donne des étapes claires pour un utilisateur lambda\n",
    "    - Tu peux inclure des commandes terminales si nécessaire\n",
    "    - Si des éléments sont manquants, préciser :\n",
    "    > Informations à compléter ou configuration nécessaire\n",
    "\n",
    "    ----------------------------------------\n",
    "    ### Fin du README\n",
    "\n",
    "    Le README doit se terminer par une section :\n",
    "\n",
    "    ### Avertissements et limitations\n",
    "    - 10 à 20 lignes maximum\n",
    "    - Mentionner limites, dépendances, risques, données manquantes, etc.\n",
    "\n",
    "    ----------------------------------------\n",
    "    ### Données fournies\n",
    "\n",
    "    ### Données fournies\n",
    "\n",
    "    Voici une **arborescence synthétique** (seulement fichiers/dossiers de code importants) :\n",
    "    -----------------\n",
    "    {tree}\n",
    "    -----------------\n",
    "\n",
    "\n",
    "    Voici des chunks (non ordonnés) :\n",
    "    -----------------\n",
    "    {chunks[:100]}\n",
    "    -----------------\n",
    "\n",
    "     Maintenant, génère le README complet en respectant toutes les consignes ci-dessus.\n",
    "    \"\"\"\n",
    "\n",
    "    # Appel au LLM (Codestral) pour la génération finale du README\n",
    "    response = client.chat.complete(\n",
    "        model=\"codestral-2508\", \n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2, # Température basse pour une réponse factuelle et stable\n",
    "        max_tokens=5000 \n",
    "    )\n",
    "\n",
    "    readme_text = response.choices[0].message.content\n",
    "\n",
    "    # ---- 3.7 Sauvegarde ----\n",
    "    # Si README.md existe, on crée README2.md\n",
    "    if os.path.exists(output_file):\n",
    "        base = os.path.dirname(output_file)\n",
    "        output_file = os.path.join(base, \"README2.md\")\n",
    "        print(\"README.md existant → génération dans README2.md\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme_text)\n",
    "\n",
    "    print(\"\\n README généré :\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f04bc",
   "metadata": {},
   "source": [
    "## Création du README de l'utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "14c23760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks : 107\n",
      "Génération des embeddings en parallèle...\n",
      "Erreur 429... Retry dans 1.29s pour un lot d'embeddings.\n",
      "Erreur 429... Retry dans 1.10s pour un lot d'embeddings.\n",
      "Indexation FAISS...\n",
      "README.md existant → génération dans README2.md\n",
      "\n",
      " README généré : D:\\33611\\Documents\\MASTER\\M2\\Open_data\\projet\\TourismEco\\README2.md\n"
     ]
    }
   ],
   "source": [
    "# --- Déclenchement du pipeline de génération ---\n",
    "# Demande à l'utilisateur de spécifier le chemin du dossier du projet.\n",
    "folder = input(\"Dossier à analyser : \").strip()\n",
    "\n",
    "# Définition du chemin de sortie par défaut\n",
    "output = os.path.join(folder, \"README.md\")\n",
    "\n",
    "# Lancement du processus RAG + génération README\n",
    "generate_readme_RAG(folder, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe4c25",
   "metadata": {},
   "source": [
    "## Guide Utilisateur et Avertissements\n",
    "\n",
    "Cette partie sert à guider l’utilisateur qui veut générer automatiquement un README pour son projet en utilisant ce notebook.\n",
    "\n",
    "### Guide d’Exécution\n",
    "\n",
    "Pour faire fonctionner la génération du README, voici les étapes à suivre :\n",
    "\n",
    "1. Préparer l’Environnement :\n",
    "    - Vérifiez que votre clé API Mistral (MISTRAL_API_KEY) est correctement configurée dans le fichier .env.\n",
    "    - Assurez-vous que toutes les bibliothèques nécessaires sont installées, par exemple : numpy, mistralai, faiss-cpu, etc.\n",
    "\n",
    "2. Lancer le Notebook :\n",
    "    - Exécutez toutes les cellules du notebook dans l’ordre, de la première (imports) jusqu’à la dernière cellule de génération.\n",
    "\n",
    "3. Indiquer le Projet à Analyser :\n",
    "    - Lorsque le notebook demande “Dossier à analyser”, copiez-collez le chemin complet du dossier racine de votre projet.\n",
    "      - Exemple :\n",
    "          - Windows : C:\\Users\\NomUtilisateur\\MonProjet\n",
    "          - Mac/Linux : /Users/Documents/MonProjet\n",
    "\n",
    "4. Suivre l’Exécution :\n",
    "Si vous voyez des erreurs 429, pas de panique !\n",
    "Ce sont des limites d’API (trop de requêtes trop rapides).\n",
    "Le système attend quelques secondes et réessaye automatiquement grâce au mécanisme de backoff exponentiel avec Jitter.\n",
    "\n",
    "Vous verrez alors des messages du type :\n",
    "\n",
    "\"Total chunks : 26\n",
    "Erreur 429… Retry dans 1.08s\n",
    "Erreur 429… Retry dans 2.26s\"\n",
    "\n",
    "Tout est normal, l’exécution continue toute seule.\n",
    "\n",
    "5. Récupérer le Résultat :\n",
    "\n",
    "   - Une fois terminé, un message indique : \"README généré : [chemin/vers/README.md]\"\n",
    "\n",
    "  - Le fichier README sera créé dans le dossier racine du projet.\n",
    "      - Si un README existait déjà, le nouveau sera nommé README2.md.\n",
    "\n",
    "### Avertissements et Limites Connues\n",
    "\n",
    "Quelques points importants à connaître pour utiliser ce système correctement :\n",
    "\n",
    "1. Qualité du Code Source :\n",
    "    - Plus votre code est clair et bien structuré (variables explicites, fonctions bien nommées…), plus le README généré sera précis et utile.\n",
    "    - Un code confus ou mal organisé donnera un README moins pertinent.\n",
    "\n",
    "2. Types de Fichiers :\n",
    "   - Le système est optimisé pour le code source (Python, JS, C++, etc.).\n",
    "   - Certains fichiers spécifiques ou binaires (images, fichiers compilés…) ne seront pas analysés correctement.\n",
    "\n",
    "1. Performance et Scalabilité :\n",
    "   - Le temps de génération dépend de la taille du projet.\n",
    "   - Pour des projets très volumineux (des milliers de fichiers), l’index FAISS en mémoire peut devenir un goulot d’étranglement, et la qualité des résultats peut diminuer.\n",
    "   - Dans ces cas, il peut être nécessaire de diviser le projet en sous-dossiers ou de générer le README par parties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc7eac",
   "metadata": {},
   "source": [
    "## Optimisation et Limites de la Concurrence\n",
    "\n",
    "### Optimisation par Concurrence : Justification\n",
    "\n",
    "L’utilisation du multithreading via **`ThreadPoolExecutor`** a pour objectif d’optimiser les tâches **I/O-Bound**, en particulier la lecture depuis le disque et les appels à l’API pour générer les embeddings. Ce choix permet de **masquer la latence** associée à ces opérations, en exploitant les temps d’attente pour effectuer d’autres traitements.\n",
    "\n",
    "Les bénéfices principaux sont les suivants :\n",
    "\n",
    "* **Lecture et Segmentation (Chunking)**\n",
    "    * Les accès disques sont lents et bloquants.\n",
    "    * Le multithreading permet de traiter et découper en *chunks* les fichiers déjà lus pendant que d’autres sont encore en cours de lecture.\n",
    "* **Génération des Embeddings**\n",
    "    * Les appels à l’API Mistral sont des opérations réseau.\n",
    "    * En envoyant plusieurs requêtes simultanément, le programme augmente le **débit global de vectorisation** et réduit les temps d’attente cumulés.\n",
    "\n",
    "En conséquence, l’architecture concurrente améliore significativement le temps total de traitement en exploitant intelligemment les périodes d’inactivité liées à l’I/O.\n",
    "\n",
    "---\n",
    "\n",
    "### Contraintes et Limites de la Parallélisation\n",
    "\n",
    "L’usage de la concurrence apporte des gains indéniables, mais plusieurs contraintes techniques empêchent une parallélisation complète du pipeline.\n",
    "\n",
    "#### Séquentialité Nécessaire\n",
    "\n",
    "Certaines tâches doivent rester strictement séquentielles pour des raisons d'intégrité :\n",
    "\n",
    "* **Indexation FAISS**\n",
    "    * L’ajout de vecteurs à l’index ne peut être parallélisé sans risque de corruption mémoire ou incohérence structurelle.\n",
    "* **Génération finale du README**\n",
    "    * La synthèse demandée au LLM est une opération unique, dépendant de l’ensemble des informations récoltées.\n",
    "\n",
    "#### Limites du Multithreading sur le CPU\n",
    "\n",
    "Le multithreading n’est performant que sur les tâches I/O-bound.\n",
    "\n",
    "* Les étapes computationnelles (ex. nettoyage de chaînes, découpage) sont limitées par le **GIL (Global Interpreter Lock)**.\n",
    "* Pour un projet principalement calculatoire, le *multiprocessing* aurait été préférable, bien que plus coûteux en mémoire.\n",
    "\n",
    "#### Scalabilité et Consommation Mémoire\n",
    "\n",
    "Lors d’un passage à grande échelle :\n",
    "\n",
    "* L’I/O cesse d’être le principal problème : c’est la **mémoire RAM** qui devient limitante.\n",
    "* L’index **FAISS** peut devenir trop volumineux.\n",
    "* Une solution distribuée, une indexation hiérarchique ou un stockage persistant serait alors nécessaire.\n",
    "\n",
    "---\n",
    "\n",
    "### Bilan\n",
    "\n",
    "Le multithreading a été appliqué là où il procure le meilleur rendement : I/O et réseau. Les phases séquentielles assurent la cohérence des données et de l’indexation, tandis que les limites du GIL et de la mémoire RAM ont guidé la conception d’une solution hybride, pragmatique et scalable dans des conditions réalistes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4feb03e",
   "metadata": {},
   "source": [
    "## Conclusion du Projet\n",
    "\n",
    "Le projet atteint pleinement son objectif : générer automatiquement un README structuré à partir de code source, grâce à une architecture **RAG (Retrieval-Augmented Generation)** combinant parsing intelligent, embeddings spécialisés (`codestral-embed`) et synthèse LLM (`codestral-2508`). Même sans commentaires explicites dans les fichiers, le système produit une documentation cohérente, exploitable et adaptée aux développeurs.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Contributions Clés et Optimisations\n",
    "\n",
    "Ce travail a permis de valider plusieurs concepts avancés :\n",
    "\n",
    "* **Optimisation Concurrente :** Le recours au **multithreading** (`ThreadPoolExecutor`) a réussi à paralléliser les opérations I/O-Bound (lecture disque et appels réseau), augmentant le débit global et l'efficacité de la vectorisation.\n",
    "* **Structuration de l'Information :** Le *chunking* intelligent à granularité adaptative et le renforcement du lien contexte/code (ajout du nom de fichier) ont permis au LLM d'établir un lien sémantique précis, améliorant la qualité du résumé fonctionnel.\n",
    "* **Robustesse du Pipeline :** La structure stricte du **prompt RAG normé** et le filtrage ciblé garantissent un format de sortie uniforme et immédiatement exploitable.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Limites Actuelles et Perspectives\n",
    "\n",
    "Malgré ces performances, les défis inhérents à la manipulation de données massives ouvrent des pistes d’évolution :\n",
    "\n",
    "* **Scalabilité et Mémoire :** La limite principale reste la capacité **mémoire RAM** nécessaire au stockage de l’index **FAISS** pour les projets massifs.\n",
    "* **Contraintes d'Architecture :** Les optimisations du threading sont restreintes aux tâches I/O (à cause du **GIL**) ; l'extension à des tâches CPU-bound nécessiterait une migration vers le *multiprocessing*.\n",
    "* **Contexte LLM :** La synthèse finale est toujours contrainte par la **fenêtre contextuelle** du modèle, limitant le volume de code analysable d’un seul bloc.\n",
    "\n",
    "---\n",
    "\n",
    "### En Synthèse\n",
    "\n",
    "Ce projet démontre qu’une architecture RAG, enrichie par des optimisations concurrentielles mesurées, constitue une approche pertinente et performante pour automatiser la documentation logicielle. Son extension à des architectures distribuées représente la prochaine étape pour des applications industrielles à grande échelle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
