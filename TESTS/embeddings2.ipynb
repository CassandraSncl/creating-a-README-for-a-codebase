{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5276e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc30834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Charger la clé depuis le fichier .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ec1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Lire fichiers .py et .ipynb\n",
    "def read_code(folder, extensions={\".py\", \".ipynb\"}, show_structure=False):\n",
    "    \"\"\"\n",
    "    Parcourt un dossier récursivement et extrait le contenu des fichiers .py et .ipynb.\n",
    "    \n",
    "    Args:\n",
    "        folder (str): chemin du dossier\n",
    "        extensions (set): extensions de fichiers à inclure\n",
    "        show_structure (bool): si True, affiche l’arborescence du dossier\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: liste de tuples (chemin_relatif, code_source)\n",
    "    \"\"\"\n",
    "    code_files = []\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(folder):\n",
    "        for f in filenames:\n",
    "            ext = os.path.splitext(f)[1].lower()\n",
    "            if ext in extensions:\n",
    "                full_path = os.path.join(dirpath, f)\n",
    "                rel_path = os.path.relpath(full_path, folder)\n",
    "                try:\n",
    "                    if ext == \".ipynb\":\n",
    "                        with open(full_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                            nb = json.load(file)\n",
    "                            code = \"\"\n",
    "                            for cell in nb.get(\"cells\", []):\n",
    "                                if cell.get(\"cell_type\") == \"code\":\n",
    "                                    code += \"\".join(cell.get(\"source\", [])) + \"\\n\"\n",
    "                            code_files.append((rel_path, code))\n",
    "                    else:\n",
    "                        with open(full_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                            code = file.read()\n",
    "                            code_files.append((rel_path, code))\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lecture {rel_path} : {e}\")\n",
    "\n",
    "    if show_structure:\n",
    "        print(\"Contenu du dossier :\")\n",
    "        for path, _ in code_files:\n",
    "            print(\" -\", path)\n",
    "\n",
    "    return code_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3a00f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Découper par fonction\n",
    "def split_functions(code):\n",
    "    pattern = r\"(def [\\w_]+\\s*\\(.*?\\):(?:\\n(?:\\s+.+))*)\"\n",
    "    funcs = re.findall(pattern, code, re.DOTALL)\n",
    "    return funcs if funcs else [code]\n",
    "\n",
    "# 3. Créer embeddings\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "def create_embeddings(chunks, batch_size=16, max_retries=5):\n",
    "    \"\"\"\n",
    "    Crée des embeddings pour une liste de textes (chunks) en batch\n",
    "    et gère les erreurs 429 avec retry exponentiel.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list of str): textes à encoder\n",
    "        batch_size (int): nombre de chunks à envoyer par requête\n",
    "        max_retries (int): nombre de tentatives en cas d'erreur 429\n",
    "    Returns:\n",
    "        embeddings (list of np.array): embeddings de chaque chunk\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                resp = client.embeddings.create(\n",
    "                    model=\"codestral-embed\",\n",
    "                    inputs=batch\n",
    "                )\n",
    "                # Ajouter chaque embedding du batch\n",
    "                for emb in resp.data:\n",
    "                    embeddings.append(np.array(emb.embedding, dtype=np.float32))\n",
    "                break  # sortir de la boucle retry si OK\n",
    "            except Exception as e:\n",
    "                # Vérifie si c'est une erreur 429\n",
    "                if \"429\" in str(e):\n",
    "                    wait = 2 ** attempt + random.random()\n",
    "                    print(f\"Erreur 429, tentative {attempt+1}/{max_retries}. Attente {wait:.2f}s\")\n",
    "                    time.sleep(wait)\n",
    "                else:\n",
    "                    raise e  # autre erreur → remonter\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "224e0f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu du dossier :\n",
      " - code3.py\n",
      " - DOSSIER 2/code2.py\n",
      " - DOSSIER 2/code4.ipynb\n",
      " - DOSSIER 1/code5.ipynb\n",
      " - DOSSIER 1/code1.py\n",
      "Erreur 429, tentative 1/5. Attente 1.25s\n",
      "README généré :\n",
      " # Projet d'Utilitaires de Fichiers et de Données\n",
      "\n",
      "## Description\n",
      "\n",
      "Ce projet fournit un ensemble d'utilitaires pour la manipulation de fichiers et la création de structures de données simples. Il inclut des fonctions pour gérer des fichiers et des dossiers, ainsi que des outils pour créer et manipuler des DataFrames pandas.\n",
      "\n",
      "## Fonctionnalités principales\n",
      "\n",
      "- **Gestion de fichiers et dossiers** :\n",
      "  - Lister les fichiers dans un dossier\n",
      "  - Créer des dossiers\n",
      "  - Créer et lire des fichiers\n",
      "  - Compter les lignes dans un fichier\n",
      "\n",
      "- **Manipulation de données** :\n",
      "  - Créer des DataFrames pandas à partir de listes\n",
      "  - Générer des exemples de DataFrames avec des valeurs aléatoires\n",
      "  - Opérations mathématiques simples avec numpy\n",
      "\n",
      "## Bibliothèques utilisées\n",
      "\n",
      "- `os` : Pour les opérations de système de fichiers\n",
      "- `pandas` : Pour la manipulation de données tabulaires\n",
      "- `numpy` : Pour les opérations mathématiques\n",
      "\n",
      "## Installation\n",
      "\n",
      "1. Clonez ce dépôt :\n",
      "   ```bash\n",
      "   git clone https://github.com/votre-utilisateur/votre-projet.git\n",
      "   cd votre-projet\n",
      "   ```\n",
      "\n",
      "2. Installez les dépendances :\n",
      "   ```bash\n",
      "   pip install pandas numpy\n",
      "   ```\n",
      "\n",
      "## Utilisation\n",
      "\n",
      "### Gestion de fichiers\n",
      "\n",
      "```python\n",
      "from votre_module import list_files, create_folder, create_file, read_file, count_lines\n",
      "\n",
      "# Lister les fichiers dans un dossier\n",
      "files = list_files('mon_dossier')\n",
      "\n",
      "# Créer un dossier\n",
      "create_folder('nouveau_dossier')\n",
      "\n",
      "# Créer un fichier\n",
      "create_file('fichier.txt', 'Contenu du fichier')\n",
      "\n",
      "# Lire un fichier\n",
      "contenu = read_file('fichier.txt')\n",
      "\n",
      "# Compter les lignes dans un fichier\n",
      "nombre_lignes = count_lines('fichier.txt')\n",
      "```\n",
      "\n",
      "### Manipulation de données\n",
      "\n",
      "```python\n",
      "from votre_module import create_dataframe, example_function, another_function, yet_another_function\n",
      "\n",
      "# Créer un DataFrame simple\n",
      "df_simple = create_dataframe(['A', 'B', 'C'], [1, 2, 3])\n",
      "\n",
      "# Créer un DataFrame avec des valeurs aléatoires\n",
      "df_aleatoire = example_function()\n",
      "\n",
      "# Opération mathématique\n",
      "resultat = another_function(16)  # Retourne 4.0\n",
      "\n",
      "# Transformation de liste\n",
      "liste_transformee = yet_another_function([1, 2, 3])  # Retourne [2, 4, 6]\n",
      "```\n",
      "\n",
      "## Structure des fichiers\n",
      "\n",
      "Le projet contient un seul module Python avec les fonctions suivantes :\n",
      "- `list_files()` : Liste les fichiers dans un dossier\n",
      "- `create_folder()` : Crée un dossier\n",
      "- `create_file()` : Crée un fichier avec du contenu\n",
      "- `read_file()` : Lit le contenu d'un fichier\n",
      "- `count_lines()` : Compte les lignes dans un fichier\n",
      "- `create_dataframe()` : Crée un DataFrame pandas\n",
      "- `example_function()` : Génère un DataFrame avec des valeurs aléatoires\n",
      "- `another_function()` : Calcule la racine carrée\n",
      "- `yet_another_function()` : Double chaque élément d'une liste\n",
      "\n",
      "## Contribution\n",
      "\n",
      "Les contributions sont les bienvenues. Veuillez ouvrir une issue pour discuter des changements majeurs avant de soumettre une pull request.\n",
      "\n",
      "## Licence\n",
      "\n",
      "[MIT](https://choosealicense.com/licenses/mit/)\n"
     ]
    }
   ],
   "source": [
    "# 4. Lire + chunker + embeddings\n",
    "folder = \"/Users/aya31/Desktop/M2 MIASHS/Calcul Paralle/creating-a-README-for-a-codebase/TESTS/TEST2\"\n",
    "all_code = read_code(folder, show_structure=True)\n",
    "\n",
    "all_chunks = []\n",
    "chunk_to_path = []\n",
    "\n",
    "for path, code in all_code:\n",
    "    chunks = split_functions(code)\n",
    "    all_chunks.extend(chunks)\n",
    "    chunk_to_path.extend([path] * len(chunks))  # pour traçabilité\n",
    "\n",
    "\n",
    "# 4 bis. Créer les embeddings à partir des chunks\n",
    "embeddings = create_embeddings(all_chunks, batch_size=8)\n",
    "\n",
    "\n",
    "# 5. Stocker dans FAISS\n",
    "dim = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# 6. Fonction pour poser une question au LLM avec RAG\n",
    "def ask_question(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Utilise le RAG : embeddings + FAISS pour récupérer du contexte,\n",
    "    puis envoie un prompt au LLM pour, par exemple, rédiger un README.\n",
    "    `query` contient directement la consigne (ex: 'Rédige un README GitHub pour ce projet').\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Créer l'embedding de la question\n",
    "    query_emb = client.embeddings.create(\n",
    "        model=\"codestral-embed\",\n",
    "        inputs=query\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # 2. Recherche des chunks les plus proches dans FAISS\n",
    "    import numpy as np\n",
    "    D, I = index.search(np.array([query_emb]), top_k)\n",
    "\n",
    "    # 3. Construire le contexte à partir des chunks sélectionnés\n",
    "    retrieved_chunks = [all_chunks[i] for i in I[0]]\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # 4. Préparer le prompt pour le LLM (sans bloc 'Question : ...')\n",
    "    prompt = (\n",
    "        \"Tu es un assistant expert en analyse de code et en rédaction de documentation technique.\\n\"\n",
    "        \"On te fournit des extraits de code provenant d'un projet.\\n\"\n",
    "        \"En te basant uniquement sur ces extraits, rédige le contenu demandé ci-dessous.\\n\\n\"\n",
    "        \"===== CONTEXTE (EXTRAITS DE CODE) =====\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        \"===== FIN DU CONTEXTE =====\\n\\n\"\n",
    "        \"===== CONSIGNE =====\\n\"\n",
    "        f\"{query}\\n\"\n",
    "        \"===== FIN DE LA CONSIGNE =====\\n\\n\"\n",
    "        \"Contraintes de rédaction :\\n\"\n",
    "        \"- Sois clair, structuré et précis.\\n\"\n",
    "        \"- Si la consigne demande un README GitHub, inclue :\\n\"\n",
    "        \"  - une description du projet,\\n\"\n",
    "        \"  - les fonctionnalités principales,\\n\"\n",
    "        \"  - les bibliothèques utilisées et à installer,\\n\"\n",
    "        \"  - les instructions d'installation et d'exécution,\\n\"\n",
    "        \"  - éventuellement la structure des fichiers si elle est déductible du code.\\n\"\n",
    "        \"- N'invente pas de fonctionnalités qui ne sont pas suggérées par le code.\\n\"\n",
    "    )\n",
    "\n",
    "    # 5. Appeler le LLM Mistral plus léger\n",
    "    response = client.chat.complete(\n",
    "        model=\"codestral-2508\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=1200\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# 6. Exemple d'utilisation\n",
    "question = \"Rédige un README (GitHub) complet pour ce dossier à partir du code fourni en contexte.\"\n",
    "answer = ask_question(question)\n",
    "print(\"README généré :\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83cf59e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object='list' data=[BaseModelCard(id='mistral-medium-2505', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-medium-2505', description='Our frontier-class multimodal model released May 2025.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-large-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-large-latest', description='Official mistral-large-latest Mistral AI model', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-medium-2508', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-medium-2508', description='Update on Mistral Medium 3 with improved capabilities.', max_context_length=131072, aliases=['mistral-medium-latest', 'mistral-medium'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-medium-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-medium-2508', description='Update on Mistral Medium 3 with improved capabilities.', max_context_length=131072, aliases=['mistral-medium-2508', 'mistral-medium'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-medium', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-medium-2508', description='Update on Mistral Medium 3 with improved capabilities.', max_context_length=131072, aliases=['mistral-medium-2508', 'mistral-medium-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='ministral-3b-2410', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='ministral-3b-2410', description=\"World's best edge model.\", max_context_length=131072, aliases=['ministral-3b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='ministral-3b-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='ministral-3b-2410', description=\"World's best edge model.\", max_context_length=131072, aliases=['ministral-3b-2410'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='ministral-8b-2410', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='ministral-8b-2410', description='Powerful edge model with extremely high performance/price ratio.', max_context_length=131072, aliases=['ministral-8b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='ministral-8b-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='ministral-8b-2410', description='Powerful edge model with extremely high performance/price ratio.', max_context_length=131072, aliases=['ministral-8b-2410'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='open-mistral-7b', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='open-mistral-7b', description='Our first dense model released September 2023.', max_context_length=32768, aliases=['mistral-tiny', 'mistral-tiny-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='mistral-tiny', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='open-mistral-7b', description='Our first dense model released September 2023.', max_context_length=32768, aliases=['open-mistral-7b', 'mistral-tiny-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='mistral-tiny-2312', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='open-mistral-7b', description='Our first dense model released September 2023.', max_context_length=32768, aliases=['open-mistral-7b', 'mistral-tiny'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='open-mistral-nemo', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo-2407', 'mistral-tiny-2407', 'mistral-tiny-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='open-mistral-nemo-2407', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo', 'mistral-tiny-2407', 'mistral-tiny-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-tiny-2407', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo', 'open-mistral-nemo-2407', 'mistral-tiny-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-tiny-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='open-mistral-nemo', description='Our best multilingual open source model released July 2024.', max_context_length=131072, aliases=['open-mistral-nemo', 'open-mistral-nemo-2407', 'mistral-tiny-2407'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-large-2411', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-large-2411', description='Our top-tier reasoning model for high-complexity tasks with the lastest version released November 2024.', max_context_length=131072, aliases=[], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='pixtral-large-2411', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='pixtral-large-2411', description='Official pixtral-large-2411 Mistral AI model', max_context_length=131072, aliases=['pixtral-large-latest', 'mistral-large-pixtral-2411'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='pixtral-large-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='pixtral-large-2411', description='Official pixtral-large-2411 Mistral AI model', max_context_length=131072, aliases=['pixtral-large-2411', 'mistral-large-pixtral-2411'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='mistral-large-pixtral-2411', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='pixtral-large-2411', description='Official pixtral-large-2411 Mistral AI model', max_context_length=131072, aliases=['pixtral-large-2411', 'pixtral-large-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='codestral-2508', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='codestral-2508', description='Our cutting-edge language model for coding released August 2025.', max_context_length=256000, aliases=['codestral-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='codestral-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='codestral-2508', description='Our cutting-edge language model for coding released August 2025.', max_context_length=256000, aliases=['codestral-2508'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='devstral-small-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='devstral-small-2507', description='Our small open-source code-agentic model.', max_context_length=131072, aliases=['devstral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'), BaseModelCard(id='devstral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='devstral-small-2507', description='Our small open-source code-agentic model.', max_context_length=131072, aliases=['devstral-small-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'), BaseModelCard(id='devstral-medium-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='devstral-medium-2507', description='Our medium code-agentic model.', max_context_length=131072, aliases=['devstral-medium-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'), BaseModelCard(id='devstral-medium-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='devstral-medium-2507', description='Our medium code-agentic model.', max_context_length=131072, aliases=['devstral-medium-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'), BaseModelCard(id='pixtral-12b-2409', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='pixtral-12b-2409', description='A 12B model with image understanding capabilities in addition to text.', max_context_length=131072, aliases=['pixtral-12b', 'pixtral-12b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='pixtral-12b', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='pixtral-12b-2409', description='A 12B model with image understanding capabilities in addition to text.', max_context_length=131072, aliases=['pixtral-12b-2409', 'pixtral-12b-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='pixtral-12b-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='pixtral-12b-2409', description='A 12B model with image understanding capabilities in addition to text.', max_context_length=131072, aliases=['pixtral-12b-2409', 'pixtral-12b'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-small-2506', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-small-2506', description='Our latest enterprise-grade small model with the latest version released June 2025.', max_context_length=131072, aliases=['mistral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-small-2506', description='Our latest enterprise-grade small model with the latest version released June 2025.', max_context_length=131072, aliases=['mistral-small-2506'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='magistral-medium-2509', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='magistral-medium-2509', description='Our frontier-class reasoning model release candidate September 2025.', max_context_length=131072, aliases=['magistral-medium-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='magistral-medium-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='magistral-medium-2509', description='Our frontier-class reasoning model release candidate September 2025.', max_context_length=131072, aliases=['magistral-medium-2509'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='magistral-small-2509', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='magistral-small-2509', description='Our efficient reasoning model released September 2025.', max_context_length=131072, aliases=['magistral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='magistral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='magistral-small-2509', description='Our efficient reasoning model released September 2025.', max_context_length=131072, aliases=['magistral-small-2509'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='voxtral-mini-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='voxtral-mini-2507', description='A mini audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-mini-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'), BaseModelCard(id='voxtral-mini-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='voxtral-mini-2507', description='A mini audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-mini-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'), BaseModelCard(id='voxtral-small-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='voxtral-small-2507', description='A small audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-small-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'), BaseModelCard(id='voxtral-small-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='voxtral-small-2507', description='A small audio understanding model released in July 2025', max_context_length=32768, aliases=['voxtral-small-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.2, TYPE='base'), BaseModelCard(id='devstral-small-2505', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='devstral-small-2505', description='Our small open-source code-agentic model.', max_context_length=131072, aliases=[], deprecation=datetime.datetime(2025, 11, 30, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='devstral-small-latest', default_model_temperature=0.0, TYPE='base'), BaseModelCard(id='magistral-small-2506', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='magistral-small-2506', description='Our efficient reasoning model released June 2025.', max_context_length=40000, aliases=[], deprecation=datetime.datetime(2025, 11, 30, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='magistral-small-latest', default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='magistral-medium-2506', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='magistral-medium-2506', description='Our frontier-class reasoning model released June 2025.', max_context_length=40960, aliases=[], deprecation=datetime.datetime(2025, 11, 30, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='magistral-medium-latest', default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='magistral-small-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='magistral-small-2507', description='Our efficient reasoning model released July 2025.', max_context_length=40960, aliases=[], deprecation=datetime.datetime(2025, 11, 30, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='magistral-small-latest', default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='magistral-medium-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='magistral-medium-2507', description='Our frontier-class reasoning model released July 2025.', max_context_length=40960, aliases=[], deprecation=datetime.datetime(2025, 11, 30, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='magistral-medium-latest', default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='mistral-small-2409', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-small-2409', description='Our latest enterprise-grade small model with the latest version v2 released September 2024. ', max_context_length=32768, aliases=[], deprecation=datetime.datetime(2025, 11, 30, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='mistral-small-latest', default_model_temperature=0.7, TYPE='base'), BaseModelCard(id='codestral-2501', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='codestral-2501', description='Our cutting-edge language model for coding released December 2024.', max_context_length=256000, aliases=['codestral-2412', 'codestral-2411-rc5'], deprecation=datetime.datetime(2025, 11, 30, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='codestral-latest', default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='codestral-2412', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='codestral-2501', description='Our cutting-edge language model for coding released December 2024.', max_context_length=256000, aliases=['codestral-2501', 'codestral-2411-rc5'], deprecation=datetime.datetime(2025, 11, 30, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='codestral-latest', default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='codestral-2411-rc5', capabilities=ModelCapabilities(completion_chat=True, completion_fim=True, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='codestral-2501', description='Our cutting-edge language model for coding released December 2024.', max_context_length=256000, aliases=['codestral-2501', 'codestral-2412'], deprecation=datetime.datetime(2025, 11, 30, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='codestral-latest', default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-small-2503', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-small-2503', description='Our latest enterprise-grade small model with the latest version released March 2025.', max_context_length=131072, aliases=[], deprecation=datetime.datetime(2025, 11, 30, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='mistral-small-latest', default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-small-2501', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=True, fine_tuning=True, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-small-2501', description='Our latest enterprise-grade small model with the latest version released January 2025. ', max_context_length=32768, aliases=[], deprecation=datetime.datetime(2025, 11, 30, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='mistral-small-latest', default_model_temperature=0.3, TYPE='base'), BaseModelCard(id='mistral-embed-2312', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-embed-2312', description='Official mistral-embed-2312 Mistral AI model', max_context_length=8192, aliases=['mistral-embed'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'), BaseModelCard(id='mistral-embed', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-embed-2312', description='Official mistral-embed-2312 Mistral AI model', max_context_length=8192, aliases=['mistral-embed-2312'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'), BaseModelCard(id='codestral-embed', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='codestral-embed', description='Official codestral-embed Mistral AI model', max_context_length=8192, aliases=['codestral-embed-2505'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'), BaseModelCard(id='codestral-embed-2505', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='codestral-embed', description='Official codestral-embed Mistral AI model', max_context_length=8192, aliases=['codestral-embed'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'), BaseModelCard(id='mistral-moderation-2411', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=True), object='model', created=1763464545, owned_by='mistralai', name='mistral-moderation-2411', description='Official mistral-moderation-2411 Mistral AI model', max_context_length=8192, aliases=['mistral-moderation-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'), BaseModelCard(id='mistral-moderation-latest', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=True), object='model', created=1763464545, owned_by='mistralai', name='mistral-moderation-2411', description='Official mistral-moderation-2411 Mistral AI model', max_context_length=8192, aliases=['mistral-moderation-2411'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=None, TYPE='base'), BaseModelCard(id='mistral-ocr-2505', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-ocr-2505', description='Official mistral-ocr-2505 Mistral AI model', max_context_length=16384, aliases=['mistral-ocr-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'), BaseModelCard(id='mistral-ocr-latest', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-ocr-2505', description='Official mistral-ocr-2505 Mistral AI model', max_context_length=16384, aliases=['mistral-ocr-2505'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'), BaseModelCard(id='mistral-ocr-2503', capabilities=ModelCapabilities(completion_chat=False, completion_fim=False, function_calling=True, fine_tuning=False, vision=True, classification=False), object='model', created=1763464545, owned_by='mistralai', name='mistral-ocr-2503', description='Official mistral-ocr-2503 Mistral AI model', max_context_length=16384, aliases=[], deprecation=datetime.datetime(2026, 3, 31, 12, 0, tzinfo=TzInfo(UTC)), deprecation_replacement_model='mistral-ocr-latest', default_model_temperature=0.0, TYPE='base'), BaseModelCard(id='voxtral-mini-transcribe-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='voxtral-mini-transcribe-2507', description='A mini transcription model released in July 2025', max_context_length=16384, aliases=['voxtral-mini-2507', 'voxtral-mini-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'), BaseModelCard(id='voxtral-mini-2507', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='voxtral-mini-transcribe-2507', description='A mini transcription model released in July 2025', max_context_length=16384, aliases=['voxtral-mini-transcribe-2507', 'voxtral-mini-latest'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base'), BaseModelCard(id='voxtral-mini-latest', capabilities=ModelCapabilities(completion_chat=True, completion_fim=False, function_calling=False, fine_tuning=False, vision=False, classification=False), object='model', created=1763464545, owned_by='mistralai', name='voxtral-mini-transcribe-2507', description='A mini transcription model released in July 2025', max_context_length=16384, aliases=['voxtral-mini-transcribe-2507', 'voxtral-mini-2507'], deprecation=None, deprecation_replacement_model=None, default_model_temperature=0.0, TYPE='base')]\n"
     ]
    }
   ],
   "source": [
    "print(client.models.list())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
